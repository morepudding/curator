# ğŸ§ª Tests Simples LLM - Mistral 7B Instruct

**Date** : 24 novembre 2025  
**ModÃ¨le** : Mistral 7B Instruct (4-bit GPTQ)  
**GPU** : GTX 1660 Super (6GB)

---

## ğŸ“ Test 1 : Hero Description (Simple)

**Objectif** : Tester gÃ©nÃ©ration description physique hÃ©ros

### Script Python : `test_llm_hero_description.py`

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

print("ğŸ”§ Chargement modÃ¨le...")
model_path = "./models/mistral-7b-instruct-gptq"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    trust_remote_code=True
)

print(f"âœ… ModÃ¨le chargÃ© sur {model.device}")

# Prompt
prompt = """<s>[INST] You are a D&D 5e character creator.

Generate a physical description for a hero named Bjorn, a human fighter.

Requirements:
- Length: 150-200 words
- Include: appearance, clothing, equipment
- Tone: Descriptive, immersive
- Style: Medieval fantasy

[/INST]"""

print("\nğŸ“ GÃ©nÃ©ration en cours...\n")
start = time.time()

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=300,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
elapsed = time.time() - start

# Extraire seulement la rÃ©ponse (aprÃ¨s [/INST])
response = result.split("[/INST]")[-1].strip()

print("=" * 60)
print("ğŸ“œ RÃ‰SULTAT :")
print("=" * 60)
print(response)
print("=" * 60)
print(f"\nâ±ï¸  Temps gÃ©nÃ©ration : {elapsed:.2f}s")
print(f"ğŸ“Š Nombre de mots : {len(response.split())}")
print(f"ğŸ’¾ VRAM utilisÃ©e : {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
```

### Commandes

```powershell
# Activer environnement
cd c:\Users\BOTTEROOOW\dispatchgenerator\curator-backend
.\venv\Scripts\Activate.ps1

# CrÃ©er le fichier
# [Copier le script ci-dessus dans test_llm_hero_description.py]

# ExÃ©cuter
python test_llm_hero_description.py
```

### âœ… CritÃ¨res Validation

- [ ] GÃ©nÃ©ration rÃ©ussie (pas d'erreur)
- [ ] Temps < 30s
- [ ] Texte cohÃ©rent (150-200 mots)
- [ ] QualitÃ© narrative (1-5) : ___
- [ ] VRAM < 5GB

---

## ğŸ“ Test 2 : Hero Lore (Moyen)

**Objectif** : Tester gÃ©nÃ©ration backstory longue

### Script Python : `test_llm_hero_lore.py`

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

print("ğŸ”§ Chargement modÃ¨le...")
model_path = "./models/mistral-7b-instruct-gptq"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    trust_remote_code=True
)

prompt = """<s>[INST] You are a D&D 5e storyteller.

Generate a complete backstory for Bjorn, a human fighter with the Soldier background.

Requirements:
- Length: 400-500 words
- Include: childhood, pivotal event, motivation, current situation
- Personality: Disciplined, protective
- Tone: Dramatic, personal
- Make it emotionally engaging

[/INST]"""

print("\nğŸ“ GÃ©nÃ©ration lore (peut prendre 30-60s)...\n")
start = time.time()

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=600,
    temperature=0.75,
    top_p=0.92,
    do_sample=True
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
elapsed = time.time() - start
response = result.split("[/INST]")[-1].strip()

print("=" * 60)
print("ğŸ“œ BACKSTORY BJORN :")
print("=" * 60)
print(response)
print("=" * 60)
print(f"\nâ±ï¸  Temps gÃ©nÃ©ration : {elapsed:.2f}s")
print(f"ğŸ“Š Nombre de mots : {len(response.split())}")
print(f"ğŸ’¾ VRAM utilisÃ©e : {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
```

### âœ… CritÃ¨res Validation

- [ ] GÃ©nÃ©ration rÃ©ussie
- [ ] Temps < 60s
- [ ] Texte cohÃ©rent (400-500 mots)
- [ ] Arc narratif complet (enfance â†’ Ã©vÃ©nement â†’ motivation)
- [ ] QualitÃ© narrative (1-5) : ___

---

## ğŸ“ Test 3 : Dialogue (AvancÃ©)

**Objectif** : Tester gÃ©nÃ©ration dialogue structurÃ©

### Script Python : `test_llm_dialogue.py`

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import time

print("ğŸ”§ Chargement modÃ¨le...")
model_path = "./models/mistral-7b-instruct-gptq"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    trust_remote_code=True
)

prompt = """<s>[INST] Generate a dialogue between Bjorn (human fighter, soldier) and the Player (village leader).

Context: First meeting
Objective: Introduce Bjorn
Number of exchanges: 6 (3 from hero, 3 from player, alternating)

Format your response as JSON:
{
  "exchanges": [
    {"order": 1, "speaker": "hero", "text": "...", "emotion": "neutral"},
    {"order": 2, "speaker": "player", "text": "..."},
    {"order": 3, "speaker": "hero", "text": "...", "emotion": "intrigued"}
  ]
}

Generate the dialogue now.
[/INST]"""

print("\nğŸ“ GÃ©nÃ©ration dialogue...\n")
start = time.time()

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=500,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
elapsed = time.time() - start
response = result.split("[/INST]")[-1].strip()

print("=" * 60)
print("ğŸ’¬ DIALOGUE GÃ‰NÃ‰RÃ‰ :")
print("=" * 60)
print(response)
print("=" * 60)

# Tenter de parser JSON
try:
    dialogue_data = json.loads(response)
    print("\nâœ… JSON VALIDE !")
    print(f"Nombre d'Ã©changes : {len(dialogue_data.get('exchanges', []))}")
except json.JSONDecodeError:
    print("\nâš ï¸  JSON INVALIDE (mais c'est normal pour un premier test)")

print(f"\nâ±ï¸  Temps gÃ©nÃ©ration : {elapsed:.2f}s")
print(f"ğŸ’¾ VRAM utilisÃ©e : {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
```

### âœ… CritÃ¨res Validation

- [ ] GÃ©nÃ©ration rÃ©ussie
- [ ] JSON valide (ou proche)
- [ ] 6 Ã©changes alternÃ©s
- [ ] Dialogue naturel
- [ ] QualitÃ© (1-5) : ___

---

## ğŸ“ Test 4 : Mission Description

**Objectif** : Tester gÃ©nÃ©ration contenu mission

### Script Python : `test_llm_mission.py`

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

print("ğŸ”§ Chargement modÃ¨le...")
model_path = "./models/mistral-7b-instruct-gptq"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    trust_remote_code=True
)

prompt = """<s>[INST] Generate a D&D mission description.

Type: Infiltration
Location: Dark Cave (north of village)
Objective: Retrieve stolen artifact
Difficulty: Medium (7/10)

Generate:
1. Mission description (200-250 words) - urgent, tense tone
2. Success text (100 words) - victorious tone
3. Failure text (100 words) - consequences tone

Format clearly with headers.
[/INST]"""

print("\nğŸ“ GÃ©nÃ©ration mission...\n")
start = time.time()

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=600,
    temperature=0.75,
    top_p=0.9,
    do_sample=True
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
elapsed = time.time() - start
response = result.split("[/INST]")[-1].strip()

print("=" * 60)
print("ğŸ—ºï¸  MISSION GÃ‰NÃ‰RÃ‰E :")
print("=" * 60)
print(response)
print("=" * 60)
print(f"\nâ±ï¸  Temps gÃ©nÃ©ration : {elapsed:.2f}s")
print(f"ğŸ“Š Nombre de mots : {len(response.split())}")
print(f"ğŸ’¾ VRAM utilisÃ©e : {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
```

### âœ… CritÃ¨res Validation

- [ ] GÃ©nÃ©ration rÃ©ussie
- [ ] Description + success + failure prÃ©sents
- [ ] Longueurs appropriÃ©es
- [ ] Tons distincts (urgence â†’ victoire â†’ Ã©chec)
- [ ] QualitÃ© (1-5) : ___

---

## ğŸ“Š Fiche RÃ©sultats Tests

| Test | Temps | VRAM | QualitÃ© (1-5) | Notes |
|------|-------|------|---------------|-------|
| Test 1 (Description) | ___s | ___GB | ___ | |
| Test 2 (Lore) | ___s | ___GB | ___ | |
| Test 3 (Dialogue) | ___s | ___GB | ___ | |
| Test 4 (Mission) | ___s | ___GB | ___ | |

---

## ğŸ¯ Prochaines Ã‰tapes

AprÃ¨s avoir exÃ©cutÃ© ces 4 tests, on pourra :
1. Ajuster paramÃ¨tres (temperature, top_p) si besoin
2. CrÃ©er prompts optimisÃ©s pour chaque type
3. Passer aux tests Stable Diffusion
